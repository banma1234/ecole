# ğŸ Ecole Project Repository

ì—ê¼´í”„ë¡œì íŠ¸ì„œ í•˜ëŠ” ëª¨ë“  ì‘ì—…ì„ ì—…ë¡œë“œ í•˜ëŠ” ë ˆí¬ì§€ ì…ë‹ˆë‹¤.

+ ì£¼ì°¨ë³„ë¡œ í”„ë¡œì íŠ¸ ë‚´ìš©ì´ ë‹¤ë¦…ë‹ˆë‹¤.

<hr/>

## Used fucntion
+ Library & Frameworks

none.

+ with

![Visual Studio Code](https://img.shields.io/badge/Visual%20Studio%20Code-0078d7.svg?style=for-the-badge&logo=visual-studio-code&logoColor=white)


<hr/>

## Project members

#### `banma1234(ë°•ë²”ìˆ˜)`


<hr/>

### 1ì£¼ì°¨
* #### python ë¬¸ì œí’€ì´
`python` ì–¸ì–´ë¥¼ ì‚¬ìš©í•´ ë°±ì¤€ ë¬¸ì œí’€ì´ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

### 2ì£¼ì°¨
* #### html & cssë¥¼ í™œìš©í•œ ìê¸°ì†Œê°œ í˜ì´ì§€
ìˆœìˆ˜ `html`ê³¼ `css`ë§Œì„ ì‚¬ìš©í•´ ìì‹ ì„ ì†Œê°œí•˜ëŠ” ê°„ë‹¨í•œ í˜ì´ì§€ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.

### 4ì£¼ì°¨
* #### Twitter ì›¹ í¬ë¡¤ë§ ë° word ë§ˆì´ë‹
`snscrape`, `wordCloud` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ `twitter`ì—ì„œ `íŠ¹ì • í‚¤ì›Œë“œ`ê°€ ë³¸ë¬¸ì— í¬í•¨ëœ íŠ¸ìœ—ì„ í¬ë¡¤ë§, í•µì‹¬ í‚¤ì›Œë“œì™€ `í•¨ê»˜ ì–¸ê¸‰ëœ ê´€ë ¨ ë‹¨ì–´`ë“¤ì„ ë¶„ì„, í•´ë‹¹ ë‹¨ì–´ì˜ ì–¸ê¸‰ ë¹ˆë„ìˆ˜ì— ë”°ë¼ `ì‹œê°í™”ëœ ìë£Œ`ë¥¼ ìƒì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

<hr/>

## ğŸ“Œ 4ì£¼ì°¨ : Web Crawling
### ğŸš—í˜„ëŒ€ì¸ì„ ìœ„í•œ 3ì¤„ìš”ì•½.
>  `snscrape`, `wordCloud` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ `twitter`ì—ì„œ `íŠ¹ì • í‚¤ì›Œë“œ`ê°€ ë³¸ë¬¸ì— í¬í•¨ëœ íŠ¸ìœ—ì„ í¬ë¡¤ë§, í•µì‹¬ í‚¤ì›Œë“œì™€ `í•¨ê»˜ ì–¸ê¸‰ëœ ê´€ë ¨ ë‹¨ì–´`ë“¤ì„ ë¶„ì„, í•´ë‹¹ ë‹¨ì–´ì˜ ì–¸ê¸‰ ë¹ˆë„ìˆ˜ì— ë”°ë¼ `ì‹œê°í™”ëœ ìë£Œ`ë¥¼ ìƒì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

* ### ì‚¬ìš©ëœ ëª¨ë“ˆ
```python
# ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
import pandas as pd

# ì›¹í¬ë¡¤ë§ ê´€ë ¨ëª¨ë“ˆ
import snscrape.modules.twitter as sntwitter
import itertools
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# wordCloud ëª¨ë“ˆ
from wordcloud import WordCloud
import matplotlib.pyplot as plt
```

<hr/>

### ğŸ‘‰ íŠ¸ìœ— ê¸ì–´ì˜¤ê¸°
íŠ¸ìœ—ì„ ê¸ì–´ì˜¤ê¸° ìœ„í•œ query ì‘ì„±ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ì´ í•„ìš”í•˜ë‹¤.
* ê²€ìƒ‰í•  í‚¤ì›Œë“œ, 
* ê²€ìƒ‰í•  ë‚ ì§œ ë²”ìœ„

ê°ê° `search_word`, `start_day`, `end_day`ì— ì €ì¥í•œ í›„ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•œë‹¤.
```python
#ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‹¨ì–´
search_word = "ì•ˆë…•"

#ê²€ìƒ‰í•˜ëŠ” ê¸°ê°„
start_day = "2022-10-01"
end_day = "2022-10-14"

search_query = search_word + ' since:' + start_day + ' until:' + end_day 
```
ì´í›„ ì‘ì„±í•œ queryë¬¸ì„ `scraped`ëª¨ë“ˆì„ í†µí•´ ì²˜ë¦¬í•´ì¤€ë‹¤. 

`sliced_scrapped_tweets`ì˜ íŒŒë¼ë¯¸í„°ë¡œ íŠ¸ìœ—ì„ ê¸ì–´ì˜¬ ëª…ë ¹ê³¼ ê¸ì–´ì˜¬ íŠ¸ìœ—ì˜ ê°œìˆ˜ë¥¼ ì „ë‹¬í•œë‹¤.
```python
#ì§€ì •í•œ ê¸°ê°„ì—ì„œ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•œ tweetë¥¼ ì·¨ë“
scraped_tweets = sntwitter.TwitterSearchScraper(search_query).get_items()
#ì²˜ìŒë¶€í„° 1000ê°œì˜ tweetsë¥¼ ì·¨ë“
sliced_scraped_tweets = itertools.islice(scraped_tweets, 1000)
```

<hr/>

### ğŸ‘‰ ë°ì´í„° ì „ì²˜ë¦¬ & ì •ë¦¬
ì´ì œ ê¸ì–´ì˜¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ì •ëˆí•  ì°¨ë¡€ì´ë‹¤. `pandas DataFrame` ëª¨ë“ˆì„ ì´ìš©í•´ ê¸ì–´ì˜¨ ë°ì´í„°ë¥¼ `pandas` í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•œë‹¤. 

ê·¸ë¦¬ê³  í•´ì‰¬íƒœê·¸/ë‹‰ë„¤ì„ì´ ì•„ë‹Œ `ë³¸ë¬¸`ì— ë‹¨ì–´ê°€ ë“±ë¡ëœ ê²½ìš°ë§Œì„ ìœ„í•´ íŠ¸ìœ—ì˜ `content` ë¼ë²¨ì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ë§Œ ê¸ì–´ì˜¤ë„ë¡ í•˜ì˜€ë‹¤.

```python
#pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(sliced_scraped_tweets)
df = df[df['content'].str.contains('ì•ˆë…•|í•˜ì´|ë°˜ê°€ì›Œ|ì•ˆë…•í•˜ì„¸ìš”')]
```
ë˜í•œ íŠ¸ìœ—ì„ ê¸ì–´ì˜¬ ë•Œ í•„ìš”ì—†ëŠ” ë°ì´í„°ëŠ” ë¯¸ë¦¬ ì œê±°í•˜ê¸° ìœ„í•´ `stop_words`. ì¦‰ `ë¶ˆìš©ì–´`ë¥¼ ì„¤ì •í–ˆë‹¤. 

ì˜ì–´ì˜ ê²½ìš° ê¸°ë³¸ì ì¸ ë¶ˆìš©ì–´ë“¤ì´ ì œê³µë˜ì§€ë§Œ í•œê¸€ì€ ê·¸ë ‡ì§€ ì•Šìœ¼ë¯€ë¡œ ë³¸ì¸ì´ ì§ì ‘ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤. 
```python
stop_words = " ~~~í•œê¸€ ë¶ˆìš©ì–´ë“¤~~~ "
stop_words=stop_words.split(' ')
```
ë¶ˆìš©ì–´ ì§€ì •ì´ ëë‚¬ë‹¤ë©´ ì‹¤ì œë¡œ íŠ¸ìœ—ì—ì„œ ë¶ˆìš©ì–´ë“¤ì„ ì‚­ì œí•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ì°¨ë¡€ì´ë‹¤.
```python
# íŠ¸ìœ„í„°ë¶„ì„ì„ ìœ„í•œ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ cleaning í•¨ìˆ˜
def CleanText(readData, Num=True, Eng=True):
    # Remove Retweets
    text = re.sub('RT @[\w_]+: ', '', readData)
    # Remove Mentions
    text = re.sub('@[\w_]+', '', text)
    # Remove or Replace URL
    text = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", ' ',
                  text)  # httpë¡œ ì‹œì‘ë˜ëŠ” url
    text = re.sub(r"[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{2,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)", ' ',
                  text)  # httpë¡œ ì‹œì‘ë˜ì§€ ì•ŠëŠ” url
    # Remove only hashtag simbol "#" because hashtag contains huge information
    text = re.sub(r'#', ' ', text)
    # Remove Garbage Words (ex. &lt, &gt, etc)
    text = re.sub('[&]+[a-z]+', ' ', text)
    # Remove Special Characters
    text = re.sub('[^0-9a-zA-Zã„±-ã…ê°€-í£]', ' ', text)
    # Remove ì¶œì²˜ by yamada
    text = re.sub(r"(ì¶œì²˜.*)", ' ', text)
    # Remove newline
    text = text.replace('\n', ' ')

    if Num is True:
        # Remove Numbers
        text = re.sub(r'\d+', ' ', text)

    if Eng is True:
        # Remove English
        text = re.sub('[a-zA-Z]', ' ', text)

    # Remove multi spacing & Reform sentence
    text = ' '.join(text.split())

    return text
```
ë³´ë©´ ì•Œê² ì§€ë§Œ ì˜¨ê°– ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì§¬ë½•ì´ ë˜ ë¬´ì²™ì´ë‚˜ ì•Œì•„ë³´ê¸° í˜ë“¤ë‹¤. ëŒ€ì¶© ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤ê³  ì•Œê³ ë§Œ ìˆìœ¼ë©´ ë˜ê² ë‹¤.
* `ë¦¬íŠ¸ìœ—`, `ë©˜ì…˜` ì‚­ì œ
* `í•´ì‰¬íƒœê·¸`, `URL` ì‚­ì œ
* `ì“°ë ˆê¸°ê°’`, `íŠ¹ìˆ˜ë¬¸ì` ì‚­ì œ
* `ì¤„ë°”ê¿ˆ`, `ì¶œì²˜` ì‚­ì œ

ë˜í•œ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë°›ì€ `Num`, `Eng`ì´ `True`ë¼ë©´ í•´ë‹¹í•˜ëŠ” ê°’(ìˆ«ì, ì•ŒíŒŒë²³)ì„ ì‚­ì œí•´ì¤€ë‹¤. 
```python
for tweet in df.content:
  cleaned_tweet = []
  # í•œê¸€ ë¶ˆìš©ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ Engì— Falseê°’ì„ ì¤€ë‹¤
  cleaned_tweet_string = CleanText(tweet, Num=True, Eng=False)
  tweet_tokens = word_tokenize(cleaned_tweet_string)
  for token in tweet_tokens:
    if token.lower() not in stop_words:
      cleaned_tweet.append(token)

  cleaned_tweets_all.append(cleaned_tweet)
```
ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¸ìœ— í•˜ë‚˜í•˜ë‚˜ë¥¼ ë‹¤ ë¶ˆëŸ¬ì™€ ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜ì— ëŒ€ì…í•´ì¤€ í›„ ì‹œê°í™” ìë£Œ ìƒì„±ì„ ìœ„í•´ `í•œì¤„ì˜ string`ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.

í•œê¸€ ë‹¨ì–´ë¥¼ ë¶„ì„í•  ì˜ˆì •ì´ë‹ˆ `Num`ì€ `True`, `Eng`ì€ `False`ë¥¼ ì „ë‹¬í•´ì¤€ë‹¤.

ì´ëŸ¬í•œ ê³¼ì •ë“¤ì„ í†µí‹€ì–´ `ì „ì²˜ë¦¬ ê³¼ì •`ì´ë¼ê³  í•œë‹¤.

<hr/>

### ğŸ‘‰ ë°ì´í„° ì‹œê°í™”
`ë°ì´í„° ì‹œê°í™”`ë€ JSON, pandasì™€ ê°™ì´ í•œëˆˆì— íŒŒì•…í•˜ê¸° í˜ë“  í˜•íƒœì˜ ë°ì´í„°ë¥¼ í•œëˆˆì— ì•Œì•„ë³´ê¸° ì‰½ê²Œ ê°€ê³µí•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤.

ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆì§€ë§Œ ì´ë²ˆì—ëŠ” `wordCloud` ëª¨ë“ˆì„ ì‚¬ìš©í•´ ì‹œê°í™” ìë£Œë¥¼ ìƒì„±í•´ë³´ë„ë¡ í•˜ì.
```python
all_words = []
for cleaned_tweet in cleaned_tweets_all:
  for word in cleaned_tweet:
    all_words.append(word)

all_words_str = ' '.join(all_words)
```
`wordCloud`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” í•œì¤„ë¡œ ë¬¶ì€ ë°ì´í„°ë¥¼ì— ë„ì›Œì“°ê¸°ë¥¼ ì¶”ê°€í•´ ë‹¤ì‹œ ê°€ê³µí•´ì£¼ì–´ì•¼ í•œë‹¤. 

ë„ì›Œì“°ê¸°ê°€ ìˆì–´ì•¼ ëª¨ë“ˆì´ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
```python
def generate_wordcloud(text): 
    wordcloud = WordCloud(
                          width=800, height=400,
                          relative_scaling = 1.0,
                          # ë¡œì»¬í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í°íŠ¸ë¥¼ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤
                          font_path='malgun',
                          # ë§ˆì°¬ê°€ì§€ë¡œ ì œê±°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì—¬ê¸°ì— ì¶”ê°€ ì…ë ¥
                          stopwords = {'to', 'of'}
                          ).generate(text)
```
ì´í›„ ì‹œê°í™” ìë£Œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•œë‹¤. `WordCloud`ì˜ ì†ì„±ê°’ì— ì´ë¯¸ì§€ì˜ íŠ¹ì„±ê°’ì„ ì „ë‹¬í•´ì¤€ë‹¤. 

ì´ë•Œ í•œê¸€ ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•´ì„  ë°˜ë“œì‹œ `í°íŠ¸ì§€ì •`ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤.

ë¡œì»¬í™˜ê²½ì—ì„œ êµ¬ë™ì‹œ ë³¸ì¸ pcì˜ `Font` í´ë”ì— ìˆëŠ” í•œê¸€ í°íŠ¸ì˜ ì´ë¦„ì„ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ë”°ë¡œ í´ë”ì˜ ì£¼ì†Œê°€ í•„ìš”í•˜ì§„ ì•Šë‹¤.
```python
    fig = plt.figure(1, figsize=(8, 4))
    plt.axis('off')
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
```
ì´í›„ ëª¨ë“ˆì„ ì‹¤í–‰í•  ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ëœë‹¤. ìœ„ì™€ ê°™ì´ ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ì‹¤í–‰ ì¦‰ì‹œ ì‚¬ì§„ì´ ì¶œë ¥ëœë‹¤.
```python
cloud.to_file('íŒŒì¼ëª…')
```
íŒŒì¼ì˜ í˜•íƒœë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ í•´ë‹¹ ì½”ë“œë¥¼ ì‚½ì…í•˜ë©´ ëœë‹¤.

<hr/>

### ğŸ‘‰ ê²°ê³¼í™”ë©´

* 2018ë…„ 10. 01 ~ 10. 14
![](https://velog.velcdn.com/images/banma1234/post/5fd18059-fc39-493e-97fe-66af0074d50d/image.png)

* 2022ë…„ 10. 01 ~ 10. 14
![](https://velog.velcdn.com/images/banma1234/post/d2171aeb-e5b8-45d9-89e7-b5a4d3c07bf9/image.png)

<hr/>


### ğŸ‘‰ ì „ì²´ì½”ë“œ
```python
import pandas as pd
import snscrape.modules.twitter as sntwitter
import itertools
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# wordCloud ëª¨ë“ˆ
from wordcloud import WordCloud
import matplotlib.pyplot as plt

#=================================================================

# reference : Dr. ì•¼ë§ˆë‹¤ ì•„í‚¤íˆì½”
# https://colab.research.google.com/drive/14D9Zu4RN_fGABf-VRGooneIdsW16QqxP?hl=ko#scrollTo=8qfKxmWS2TCL

#=================================================================

# íŠ¸ìœ„í„°ë¶„ì„ì„ ìœ„í•œ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ cleaning í•¨ìˆ˜
def CleanText(readData, Num=True, Eng=True):
    # Remove Retweets
    text = re.sub('RT @[\w_]+: ', '', readData)
    # Remove Mentions
    text = re.sub('@[\w_]+', '', text)
    # Remove or Replace URL
    text = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", ' ',
                  text)  # httpë¡œ ì‹œì‘ë˜ëŠ” url
    text = re.sub(r"[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{2,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)", ' ',
                  text)  # httpë¡œ ì‹œì‘ë˜ì§€ ì•ŠëŠ” url
    # Remove only hashtag simbol "#" because hashtag contains huge information
    text = re.sub(r'#', ' ', text)
    # Remove Garbage Words (ex. &lt, &gt, etc)
    text = re.sub('[&]+[a-z]+', ' ', text)
    # Remove Special Characters
    text = re.sub('[^0-9a-zA-Zã„±-ã…ê°€-í£]', ' ', text)
    # Remove ì¶œì²˜ by yamada
    text = re.sub(r"(ì¶œì²˜.*)", ' ', text)
    # Remove newline
    text = text.replace('\n', ' ')

    if Num is True:
        # Remove Numbers
        text = re.sub(r'\d+', ' ', text)

    if Eng is True:
        # Remove English
        text = re.sub('[a-zA-Z]', ' ', text)

    # Remove multi spacing & Reform sentence
    text = ' '.join(text.split())

    return text

#=================================================================


#ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‹¨ì–´
search_word = "ì•ˆë…•"

#ê²€ìƒ‰í•˜ëŠ” ê¸°ê°„
start_day = "2022-10-01"
end_day = "2022-10-14"

search_query = search_word + ' since:' + start_day + ' until:' + end_day 

#ì§€ì •í•œ ê¸°ê°„ì—ì„œ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•œ tweetë¥¼ ì·¨ë“
scraped_tweets = sntwitter.TwitterSearchScraper(search_query).get_items()

#ì²˜ìŒë¶€í„° 1000ê°œì˜ tweetsë¥¼ ì·¨ë“
sliced_scraped_tweets = itertools.islice(scraped_tweets, 1000)

#pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(sliced_scraped_tweets)
df = df[df['content'].str.contains('ì•ˆë…•|í•˜ì´|ë°˜ê°€ì›Œ|ì•ˆë…•í•˜ì„¸ìš”')]

stop_words = " ~~~í•œê¸€ ë¶ˆìš©ì–´~~~"
stop_words=stop_words.split(' ')

# tweet í•˜ë‚˜í•˜ë‚˜ ë¶ˆëŸ¬ì˜¤ê³  stopwords ì œê±°
cleaned_tweets_all = []

for tweet in df.content:
  cleaned_tweet = []
  # í•œê¸€ ë¶ˆìš©ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ Engì— Falseê°’ì„ ì¤€ë‹¤
  cleaned_tweet_string = CleanText(tweet, Num=True, Eng=False)
  tweet_tokens = word_tokenize(cleaned_tweet_string)
  for token in tweet_tokens:
    if token.lower() not in stop_words:
      cleaned_tweet.append(token)

  cleaned_tweets_all.append(cleaned_tweet)

# print(cleaned_tweet)


#===============================================================

# wordCloud ìƒì„±
def generate_wordcloud(text): 
    wordcloud = WordCloud(
                          width=800, height=400,
                          relative_scaling = 1.0,
                          font_path='malgun', # coLabì´ ì•„ë‹Œ ë¡œì»¬í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í°íŠ¸ë¥¼ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤
                          stopwords = {'to', 'of'} #ì œê±°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì—¬ê¸°ì— ì…ë ¥
                          ).generate(text)
    
    fig = plt.figure(1, figsize=(8, 4))
    plt.axis('off')
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()

all_words = []
for cleaned_tweet in cleaned_tweets_all:
  for word in cleaned_tweet:
    all_words.append(word)

all_words_str = ' '.join(all_words)

generate_wordcloud(all_words_str)
```

<hr/>

>* #### Reference By....
### Dr. ì•¼ë§ˆë‹¤ ì•„í‚¤íˆì½”
https://colab.research.google.com/drive/14D9Zu4RN_fGABf-VRGooneIdsW16QqxP?hl=ko#scrollTo=8qfKxmWS2TCL
